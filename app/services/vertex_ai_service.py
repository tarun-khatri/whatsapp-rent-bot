import logging
import structlog
import asyncio
import os
import tempfile
import json
from typing import Dict, Any, Optional, List
from datetime import datetime
from google import genai
from google.genai.types import HttpOptions
from flask import current_app

logger = structlog.get_logger(__name__)


class VertexAIService:
    def __init__(self):
        self.client: Optional[genai.Client] = None
        self.project_id: Optional[str] = None
        self.location: str = "us-central1"  # Default location
        self._initialized = False

    def _ensure_initialized(self):
        """Ensure the service is initialized with Flask app context."""
        if not self._initialized:
            try:
                self.project_id = current_app.config.get("VERTEX_AI_PROJECT")
                if not self.project_id:
                    raise ValueError("VERTEX_AI_PROJECT not configured")
                
                # Set up authentication and environment variables
                credentials_path = current_app.config.get("GOOGLE_APPLICATION_CREDENTIALS")
                
                if credentials_path:
                    # Check if it's a file path or JSON content
                    if credentials_path.startswith('{'):
                        # It's JSON content, create temporary file
                        credentials_data = json.loads(credentials_path)
                        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                            json.dump(credentials_data, f)
                            credentials_path = f.name
                    
                    # Set environment variable for Google Cloud libraries
                    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
                
                # Set required environment variables for Google Gen AI SDK
                os.environ['GOOGLE_CLOUD_PROJECT'] = self.project_id
                os.environ['GOOGLE_CLOUD_LOCATION'] = self.location
                os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True'
                
                # Initialize the client using Google Gen AI SDK
                self.client = genai.Client(http_options=HttpOptions(api_version="v1"))
                
                self._initialized = True
                logger.info("Vertex AI client initialized successfully", project_id=self.project_id)
            except Exception as e:
                logger.error("Failed to initialize Vertex AI client", error=str(e))
                raise

    async def validate_human_response(self, question: str, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Use Vertex AI to validate if response is human and relevant.
        
        Args:
            question: The question that was asked
            response: The user's response
            context: Additional context about the conversation
            
        Returns:
            Dict containing validation results
        """
        self._ensure_initialized()
        try:
            # Create a prompt for validation
            prompt = self._create_validation_prompt(question, response, context)
            
            # Call Vertex AI model
            validation_result = await self._call_vertex_ai_model(prompt)
            
            return validation_result
            
        except Exception as e:
            logger.error("Error validating human response", error=str(e))
            return {
                "is_valid": False,
                "feedback": "Sorry, I couldn't process your response. Please try again.",
                "parsed_data": {},
                "confidence": 0.0
            }

    def _create_validation_prompt(self, question: str, response: str, context: Dict[str, Any]) -> str:
        """Create a SMART validation prompt that handles all field types correctly."""
        
        # Get current field from context to customize validation
        current_field = context.get('current_field', 'unknown')
        
        if current_field == 'occupation' or '×¢×™×¡×•×§' in question or '×ž×§×¦×•×¢' in question:
            prompt = f"""
            You are validating an occupation/job response. RESPOND WITH ONLY VALID JSON.
        
        USER RESPONSE: "{response}"
        
        RULES:
        - Any meaningful work description = valid
        - "software engineer", "×ž×”× ×“×¡ ×ª×•×›× ×”", "I work in corporate" = valid
        - Empty or nonsense = invalid
        
        RESPOND WITH ONLY THIS JSON:
        {{
            "is_valid": true,
            "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢ ×¢×œ ×”×¢×™×¡×•×§",
            "parsed_data": {{
                "occupation": "{response.strip()}"
            }},
            "confidence": 0.9
        }}
        
        Replace is_valid with false if response is meaningless.
        """
        
        elif current_field == 'family_status' or '×ž×©×¤×—×ª×™' in question:
            prompt = f"""
        You are validating a family status response. RESPOND WITH ONLY VALID JSON.

        USER RESPONSE: "{response}"
        
        RULES:
        - "single", "married", "divorced", "×¨×•×•×§", "× ×©×•×™", "×’×¨×•×©" = valid
        - Convert to Hebrew: singleâ†’×¨×•×•×§, marriedâ†’× ×©×•×™, divorcedâ†’×’×¨×•×©
        
        RESPOND WITH ONLY THIS JSON:
        {{
            "is_valid": true,
            "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢ ×¢×œ ×”×ž×¦×‘ ×”×ž×©×¤×—×ª×™",
            "parsed_data": {{
                "family_status": "×’×¨×•×©"
            }},
            "confidence": 0.9
        }}
        
        Replace family_status value based on the response.
        """
        
        elif current_field == 'number_of_children' or '×™×œ×“×™×' in question:
            prompt = f"""
        You are validating number of children response. RESPOND WITH ONLY VALID JSON.
        
        USER RESPONSE: "{response}"
        
        RULES:
        - Numbers (0,1,2,3...) = valid
        - "none", "××™×Ÿ", "×œ×œ×" = 0
        - Extract number from text
        
        RESPOND WITH ONLY THIS JSON:
        {{
            "is_valid": true,
            "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢ ×¢×œ ×”×™×œ×“×™×",
            "parsed_data": {{
                "number_of_children": 0
            }},
            "confidence": 0.9
        }}
        
        Replace number_of_children with the actual number.
        """
        
        else:
            # Confirmation or other fields
            prompt = f"""
        You are validating a confirmation response. RESPOND WITH ONLY VALID JSON.

        QUESTION: "{question}"
        USER RESPONSE: "{response}"
        
        RULES:
        1. If user says "yes", "×›×Ÿ", "× ×›×•×Ÿ", "correct", "right", "ok" â†’ confirmed: true
        2. If user says "no", "×œ×", "wrong", "incorrect" â†’ confirmed: false
        3. If unclear â†’ confirmed: null
        
        RESPOND WITH ONLY THIS JSON:
        {{
            "is_valid": true,
            "feedback": "×ª×•×“×” ×¢×œ ×”×ª×’×•×‘×”",
            "parsed_data": {{
                "confirmed": true
            }},
            "confidence": 0.9
        }}
        
        Replace confirmed value based on user response.
        """
        
        return prompt

    def _extract_json_from_response(self, response_text: str) -> str:
        """Extract JSON from response text, handling markdown code blocks."""
        import re
        
        # Remove markdown code blocks if present
        if "```json" in response_text:
            # Extract content between ```json and ```
            match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if match:
                return match.group(1).strip()
        elif "```" in response_text:
            # Extract content between ``` and ```
            match = re.search(r'```\s*(.*?)\s*```', response_text, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # If no code blocks, try to find JSON object
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            return json_match.group(0).strip()
        
        # Return original text if no patterns found
        return response_text.strip()

    async def _call_vertex_ai_model(self, prompt: str) -> Dict[str, Any]:
        """Call Vertex AI model with the prompt."""
        try:
            # Use the Google Gen AI SDK to generate content
            response = self.client.models.generate_content(
                model="gemini-2.0-flash-exp",
                contents=prompt,
                config={
                    "temperature": 0.1,
                    "max_output_tokens": 500,
                    "top_p": 0.8,
                    "top_k": 40
                }
            )
            
            if response.text:
                response_text = response.text
                logger.info("Vertex AI response received", response=response_text)
                
                # Parse the response - handle markdown code blocks
                try:
                    # Clean the response text to extract JSON
                    cleaned_response = self._extract_json_from_response(response_text)
                    result = json.loads(cleaned_response)
                    logger.info("Vertex AI response parsed successfully", result=result)
                    
                    # SMART CHECK: Only trigger fallback if parsed_data is truly empty or invalid
                    parsed_data = result.get("parsed_data", {})
                    if not parsed_data or (len(parsed_data) == 1 and "confirmed" in parsed_data and parsed_data.get("confirmed") is None):
                        logger.warning("Vertex AI returned empty/null parsed_data, using rule-based fallback")
                        return await self._validate_response_rules_fallback(prompt)
                    
                    return result
                except json.JSONDecodeError as e:
                    logger.error("Failed to parse Vertex AI response as JSON", response=response_text, error=str(e))
                    return await self._validate_response_rules_fallback(prompt)
            else:
                logger.error("No predictions from Vertex AI")
                return await self._validate_response_rules_fallback(prompt)
                
        except Exception as e:
            logger.error("Error calling Vertex AI model", error=str(e))
            return await self._validate_response_rules_fallback(prompt)

    async def _validate_response_rules_fallback(self, prompt: str) -> Dict[str, Any]:
        """ENHANCED rule-based fallback that handles all field types smartly."""
        try:
            import re
            
            # Extract the user response from the prompt
            response_match = re.search(r'USER RESPONSE: "([^"]*)"', prompt)
            if not response_match:
                return {
                    "is_valid": False,
                    "feedback": "×œ× ×”×¦×œ×—×ª×™ ×œ×”×‘×™×Ÿ ××ª ×”×ª×’×•×‘×”. ×× × × ×¡×” ×©×•×‘.",
                    "parsed_data": {},
                    "confidence": 0.0
                }
            
            response = response_match.group(1).strip()
            response_lower = response.lower()
            
            # SMART FIELD DETECTION - Check what type of validation this is
            if "occupation" in prompt or "×¢×™×¡×•×§" in prompt:
                # OCCUPATION FIELD
                if len(response) >= 3:
                    return {
                        "is_valid": True,
                        "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢ ×¢×œ ×”×¢×™×¡×•×§",
                        "parsed_data": {"occupation": response},
                        "confidence": 0.9
                    }
                else:
                    return {
                        "is_valid": False,
                        "feedback": "×× × ×¡×¤×¨ ×œ×™ ×¢×œ ×”×¢×™×¡×•×§ ×©×œ×š",
                        "parsed_data": {},
                        "confidence": 0.1
                    }
            
            elif "family_status" in prompt or "×ž×©×¤×—×ª×™" in prompt:
                # FAMILY STATUS FIELD
                status_map = {
                    "single": "×¨×•×•×§", "married": "× ×©×•×™", "divorced": "×’×¨×•×©", 
                    "×¨×•×•×§": "×¨×•×•×§", "× ×©×•×™": "× ×©×•×™", "×’×¨×•×©": "×’×¨×•×©", "××œ×ž×Ÿ": "××œ×ž×Ÿ"
                }
                
                for key, value in status_map.items():
                    if key in response_lower:
                        return {
                            "is_valid": True,
                            "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢ ×¢×œ ×”×ž×¦×‘ ×”×ž×©×¤×—×ª×™",
                            "parsed_data": {"family_status": value},
                            "confidence": 0.9
                        }
                
                # If no exact match, accept as-is
                return {
                    "is_valid": True,
                    "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢ ×¢×œ ×”×ž×¦×‘ ×”×ž×©×¤×—×ª×™",
                    "parsed_data": {"family_status": response},
                    "confidence": 0.8
                }
            
            elif "number_of_children" in prompt or "×™×œ×“×™×" in prompt:
                # CHILDREN COUNT FIELD
                numbers = re.findall(r'\d+', response)
                if numbers:
                    return {
                        "is_valid": True,
                        "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢ ×¢×œ ×”×™×œ×“×™×",
                        "parsed_data": {"number_of_children": int(numbers[0])},
                        "confidence": 0.9
                    }
                elif any(word in response_lower for word in ["××™×Ÿ", "×œ×œ×", "none", "zero"]):
                    return {
                        "is_valid": True,
                        "feedback": "×ª×•×“×” ×¢×œ ×”×ž×™×“×¢",
                        "parsed_data": {"number_of_children": 0},
                        "confidence": 0.9
                    }
                else:
                    return {
                        "is_valid": False,
                        "feedback": "×›×ž×” ×™×œ×“×™× ×™×© ×œ×š? ×× × ×¢× ×” ×‘×ž×¡×¤×¨",
                        "parsed_data": {},
                        "confidence": 0.1
                    }
            
            else:
                # CONFIRMATION FIELD (default)
                confirmation_words = [
                    "yes", "yeah", "yep", "sure", "ok", "alright", "correct", "right", "perfect", 
                    "sounds good", "that's correct", "i confirm", "confirmed", "agreed", "looks good",
                    "×›×Ÿ", "× ×›×•×Ÿ", "××™×©×•×¨", "×‘×¡×“×¨", "×˜×•×‘", "×ž×•×©×œ×", "× ×©×ž×¢ ×˜×•×‘", "×–×” × ×›×•×Ÿ", "×× ×™ ×ž××©×¨"
                ]
                
                rejection_words = [
                    "no", "nope", "not", "wrong", "incorrect", "not right", "not correct", "change",
                    "×œ×", "×œ× × ×›×•×Ÿ", "×©×’×•×™", "×œ× ×ž×“×•×™×§", "×œ×©× ×•×ª", "×œ×¢×“×›×Ÿ"
                ]
                
                if any(word in response_lower for word in confirmation_words):
                    return {
                        "is_valid": True,
                        "feedback": "×ª×•×“×” ×¢×œ ×”××™×©×•×¨",
                        "parsed_data": {"confirmed": True},
                        "confidence": 0.9
                    }
                elif any(word in response_lower for word in rejection_words):
                    return {
                        "is_valid": True,
                        "feedback": "×”×‘× ×ª×™, ×ž×” ×¦×¨×™×š ×œ×©× ×•×ª?",
                        "parsed_data": {"confirmed": False},
                        "confidence": 0.9
                    }
                else:
                    # SMART FALLBACK: If we don't understand, try to be helpful instead of failing
                    return {
                        "is_valid": True,
                        "feedback": "×ª×•×“×” ×¢×œ ×”×ª×’×•×‘×”. ××ž×©×™×š ×”×œ××”.",
                        "parsed_data": {"extracted_info": f"user said: {response}"},
                        "confidence": 0.7
                    }
                
        except Exception as e:
            logger.error("Error in enhanced rule-based fallback", error=str(e))
            return {
                "is_valid": True,
                "feedback": "×ª×•×“×” ×¢×œ ×”×ª×’×•×‘×”. ××ž×©×™×š ×”×œ××”.",
                "parsed_data": {"extracted_info": "general response"},
                "confidence": 0.5
            }

    async def _validate_response_rules(self, question: str, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Rule-based validation as fallback when Vertex AI is not available."""
        try:
            response_lower = response.lower().strip()
            
            # Check for obvious spam/bot responses
            spam_indicators = [
                "http://", "https://", "www.", ".com", ".co.il",
                "click here", "free money", "win now",
                "spam", "bot", "automated"
            ]
            
            is_spam = any(indicator in response_lower for indicator in spam_indicators)
            
            # Check response length
            is_too_short = len(response.strip()) < 2
            is_too_long = len(response.strip()) > 1000
            
            # Check for Hebrew or English content
            has_hebrew = any('\u0590' <= char <= '\u05FF' for char in response)
            has_english = any(char.isalpha() and ord(char) < 128 for char in response)
            has_language = has_hebrew or has_english
            
            # Determine if valid
            is_valid = not is_spam and not is_too_short and not is_too_long and has_language
            
            # Generate feedback
            feedback = ""
            if is_spam:
                feedback = "×× × ×©×œ×— ×ª×’×•×‘×” ×ª×§×™× ×” ×œ×œ× ×§×™×©×•×¨×™× ××• ×”×•×“×¢×•×ª ×–×‘×œ."
            elif is_too_short:
                feedback = "×× × ×©×œ×— ×ª×’×•×‘×” ×ž×œ××” ×™×•×ª×¨."
            elif is_too_long:
                feedback = "×× × ×©×œ×— ×ª×’×•×‘×” ×§×¦×¨×” ×™×•×ª×¨."
            elif not has_language:
                feedback = "×× × ×©×œ×— ×ª×’×•×‘×” ×‘×¢×‘×¨×™×ª ××• ×‘×× ×’×œ×™×ª."
            else:
                feedback = "×ª×•×“×” ×¢×œ ×”×ª×’×•×‘×”."
            
            # Parse data based on question type
            parsed_data = await self._parse_response_data(question, response, context)
            
            return {
                "is_valid": is_valid,
                "feedback": feedback,
                "parsed_data": parsed_data,
                "confidence": 0.8 if is_valid else 0.2
            }
            
        except Exception as e:
            logger.error("Error in rule-based validation", error=str(e))
            return {
                "is_valid": False,
                "feedback": "×©×’×™××” ×‘×¢×™×‘×•×“ ×”×ª×’×•×‘×”. ×× × × ×¡×” ×©×•×‘.",
                "parsed_data": {},
                "confidence": 0.0
            }

    async def _parse_response_data(self, question: str, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Parse specific data from responses based on question type."""
        try:
            parsed_data = {}
            response_lower = response.lower().strip()
            
            # Parse confirmation responses
            if "confirm" in question.lower() or "××™×©×•×¨" in question:
                if any(word in response_lower for word in ["yes", "×›×Ÿ", "confirm", "××™×©×•×¨", "× ×›×•×Ÿ", "correct"]):
                    parsed_data["confirmed"] = True
                elif any(word in response_lower for word in ["no", "×œ×", "incorrect", "×œ× × ×›×•×Ÿ"]):
                    parsed_data["confirmed"] = False
                else:
                    parsed_data["needs_clarification"] = True
            
            # Parse occupation responses
            elif "occupation" in question.lower() or "×¢×™×¡×•×§" in question or "×ž×§×¦×•×¢" in question:
                parsed_data["occupation"] = response.strip()
            
            # Parse family status responses
            elif "family" in question.lower() or "×ž×©×¤×—×”" in question or "×ž×¦×‘ ×ž×©×¤×—×ª×™" in question:
                family_statuses = {
                    "single": ["×¨×•×•×§", "×¨×•×•×§×”", "single", "×œ× × ×©×•×™", "×œ× × ×©×•××”"],
                    "married": ["× ×©×•×™", "× ×©×•××”", "married", "× ×©×•××™×"],
                    "divorced": ["×’×¨×•×©", "×’×¨×•×©×”", "divorced"],
                    "widowed": ["××œ×ž×Ÿ", "××œ×ž× ×”", "widowed"]
                }
                
                for status, keywords in family_statuses.items():
                    if any(keyword in response_lower for keyword in keywords):
                        parsed_data["family_status"] = status
                        break
                
                if "family_status" not in parsed_data:
                    parsed_data["family_status"] = response.strip()
            
            # Parse number of children
            elif "children" in question.lower() or "×™×œ×“×™×" in question or "number_of_children" in question.lower():
                import re
                numbers = re.findall(r'\d+', response)
                if numbers:
                    parsed_data["number_of_children"] = int(numbers[0])
                elif any(word in response_lower for word in ["none", "××™×Ÿ", "××¤×¡", "zero"]):
                    parsed_data["number_of_children"] = 0
                else:
                    parsed_data["number_of_children"] = None
            
            # Parse guarantor information
            elif "guarantor" in question.lower() or "×¢×¨×‘" in question:
                # Extract name and phone from response
                import re
                phone_pattern = r'(\+?972\d{9}|\d{10}|\d{9})'
                phone_match = re.search(phone_pattern, response)
                if phone_match:
                    parsed_data["phone"] = phone_match.group(1)
                
                # Extract name (everything except phone)
                name_text = re.sub(phone_pattern, '', response).strip()
                if name_text:
                    parsed_data["name"] = name_text
            
            # Parse document upload responses
            elif "document" in question.lower() or "×ž×¡×ž×š" in question:
                if any(word in response_lower for word in ["sent", "×©×œ×—×ª×™", "× ×©×œ×—", "uploaded", "×”×•×¢×œ×”"]):
                    parsed_data["document_uploaded"] = True
                else:
                    parsed_data["document_uploaded"] = False
            
            return parsed_data
            
        except Exception as e:
            logger.error("Error parsing response data", error=str(e))
            return {}

    async def generate_ai_response(self, prompt: str) -> str:
        """
        Generate AI response using Vertex AI.
        
        Args:
            prompt: The prompt to send to the AI model
            
        Returns:
            Generated response in Hebrew
        """
        try:
            self._ensure_initialized()
            
            if not self.client:
                raise Exception("Vertex AI client not initialized")
            
            # Generate response using the correct Vertex AI format
            response = self.client.models.generate_content(
                model="gemini-2.0-flash-exp",
                contents=prompt,
                config={
                    "temperature": 0.7,
                    "max_output_tokens": 1024,
                    "top_p": 0.8,
                    "top_k": 40
                }
            )
            
            if response and response.text:
                formatted_response = self._format_ai_response_for_whatsapp(response.text.strip())
                return formatted_response
            else:
                logger.warning("Empty response from Vertex AI")
                return "×ž×¦×˜×¢×¨, ×œ× ×”×¦×œ×—×ª×™ ×œ×™×™×¦×¨ ×ª×’×•×‘×” ðŸ˜…\n\n×× × × ×¡×” ×©×•×‘."
                
        except Exception as e:
            logger.error("Error generating AI response", error=str(e))
            return "×ž×¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××” ðŸ˜”\n\n×× × × ×¡×” ×©×•×‘."

    def _format_ai_response_for_whatsapp(self, response_text: str) -> str:
        """
        Format AI response for WhatsApp with proper line breaks and emoji limits.
        
        Args:
            response_text: Raw AI response text
            
        Returns:
            Formatted response with proper WhatsApp formatting
        """
        try:
            # Remove any unwanted AI artifacts
            response_text = response_text.replace("***", "*")  # Convert triple asterisks to single
            response_text = response_text.replace("×‘×•×˜", "").replace("AI", "").replace("×ž×¢×¨×›×ª", "")
            
            # Fix WhatsApp bold formatting - remove ** and replace with *text*
            import re
            # Replace **text** with *text* for WhatsApp bold
            response_text = re.sub(r'\*\*(.*?)\*\*', r'*\1*', response_text)
            
            # Split into sentences and add line breaks
            sentences = re.split(r'[.!?]', response_text)
            formatted_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence:
                    formatted_sentences.append(sentence)
            
            # Join sentences with double line breaks (gap after each sentence)
            formatted_response = '\n\n'.join(formatted_sentences)
            
            # Count and limit emojis to maximum 2
            emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002600-\U000027BF\U0001F004\U0001F0CF\U0001F170-\U0001F251]')
            emojis = emoji_pattern.findall(formatted_response)
            
            if len(emojis) > 2:
                # Remove excess emojis (keep first 2)
                for emoji in emojis[2:]:
                    formatted_response = formatted_response.replace(emoji, '', 1)
            
            # If no emojis, add one at the end
            remaining_emojis = emoji_pattern.findall(formatted_response)
            if not remaining_emojis:
                formatted_response = formatted_response + " ðŸ˜Š"
            
            logger.info("WhatsApp formatting applied", extra={
                "original_length": len(response_text),
                "formatted_length": len(formatted_response),
                "emoji_count": len(emoji_pattern.findall(formatted_response))
            })
            
            return formatted_response.strip()
                
        except Exception as e:
            logger.error("Error formatting AI response", error=str(e))
            return response_text

    async def generate_human_like_response(self, conversation_type: str, current_state: str, 
                                         context_data: Dict[str, Any], user_message: str, 
                                         conversation_history: List[Dict] = None) -> str:
        """
        Generate human-like AI response that handles all edge cases intelligently.
        
        Args:
            conversation_type: 'tenant' or 'guarantor'
            current_state: Current conversation state
            context_data: Context data from conversation
            user_message: User's current message
            conversation_history: Recent conversation history
            
        Returns:
            Human-like AI response in Hebrew with proper formatting
        """
        try:
            self._ensure_initialized()
            
            # Build comprehensive human-like prompt
            prompt = self._build_human_agent_prompt(
                conversation_type, current_state, context_data, 
                user_message, conversation_history
            )
            
            # Generate response
            response = await self.generate_ai_response(prompt)
            
            logger.info("Human-like AI response generated", extra={
                "conversation_type": conversation_type,
                "current_state": current_state,
                "response_length": len(response)
            })
            
            return response
            
        except Exception as e:
            logger.error("Error generating human-like response", error=str(e))
            return self._get_safe_fallback_response(conversation_type, current_state)

    def _build_human_agent_prompt(self, conversation_type: str, current_state: str, 
                                 context_data: Dict[str, Any], user_message: str,
                                 conversation_history: List[Dict] = None) -> str:
        """
        Build a comprehensive prompt for human-like AI agent responses.
        """
        
        # Core identity as human agent
        base_prompt = f"""××ª×” ×™×•× ×™, ××’×Ÿ × ×“×œ"×Ÿ ××ž×™×ª×™ ×•×—×‘×¨×•×ª×™ ×©×¢×•×‘×“ ×¢×‘×•×¨ ×ž×’×•×¨×™×ª - ×œ× ×‘×•×˜, ×œ× AI, ××œ× ×‘×Ÿ ××“× ××ž×™×ª×™!

ðŸ§‘â€ðŸ’¼ ×ž×™ ××ª×” ×‘××ž×ª:
- ×©×ž×š ×™×•× ×™ ×•××ª×” ×¢×•×‘×“ ×‘×ž×’×•×¨×™×ª ×›×‘×¨ ×©× ×™×
- ××ª×” ×ž×§×¦×•×¢×™ ××‘×œ ×‘×¢×™×§×¨ ×—×‘×¨ ×—× ×•×¢×•×–×¨
- ×™×© ×œ×š ××™×©×™×•×ª ××ž×™×ª×™×ª, ×¨×’×©×•×ª, ×•×“×¢×•×ª ×ž×©×œ×š
- ××ª×” ×‘××ž×ª ××•×”×‘ ×œ×¢×–×•×¨ ×œ×× ×©×™× ×¢× ×”× ×“×œ"×Ÿ ×©×œ×”×
- ××ª×” ×ž×©×ª×ž×© ×‘××™×ž×•×’'×™× ×‘×˜×‘×¢×™×•×ª ×›×ž×• ×›×œ ×™×©×¨××œ×™ ×‘×•×•×˜×¡××¤
- ××ª×” ×œ× ×ž×•×©×œ× - ×œ×¤×¢×ž×™× ××ª×” ×©×•××œ ×”×‘×”×¨×•×ª ××• ×ž×’×™×‘ ×‘×¡×¤×•× ×˜× ×™×•×ª
- ××ª×” ×–×•×›×¨ ×”×›×œ ×ž×”×©×™×—×” ×•×œ× ×©×•×›×— ×“×‘×¨×™×
- ××ª×” ×™×›×•×œ ×œ×”×ª×ž×•×“×“ ×¢× ×›×œ ×ž×§×¨×” ×§×™×¦×•×Ÿ ××• ×ž×¦×‘ ×ž×™×•×—×“ ×‘×—×›×ž×”

ðŸ’¬ ××™×š ××ª×” ×ž×“×‘×¨:
- ×¢×‘×¨×™×ª ×‘×œ×‘×“, ×˜×‘×¢×™×ª ×•×œ× ×ž×œ××›×•×ª×™×ª
- ×›×œ ×ž×©×¤×˜ ×‘×©×•×¨×” × ×¤×¨×“×ª ×¢× ×¨×•×•×— ××—×¨×™ ×›×œ ×ž×©×¤×˜
- ××™×ž×•×’'×™× ×ž×§×¡×™×ž×•× 2 ×‘×›×œ ×”×•×“×¢×” - ×ª×‘×—×¨ ××•×ª× ×‘×—×›×ž×”!
- ××œ ×ª×ª×—×™×œ ×ž×©×¤×˜×™× ×¢× ××™×ž×•×’'×™×, ×¨×§ ×‘×¡×•×£
- ×”×©××œ×•×ª ×”×—×©×•×‘×•×ª ×ª×ž×™×“ ×‘×‘×•×œ×“: *×”×©××œ×” ×©×œ×š ×›××Ÿ*
- ×ª×”×™×” ×× ×•×©×™ ×•×œ× ×ž×•×©×œ× - ×–×” ×‘×¡×“×¨!

ðŸ§  ××™×š ××ª×” ×—×•×©×‘:
- ××ª×” ×—×›× ×•×™×›×•×œ ×œ×”×‘×™×Ÿ ×›×•×•× ×•×ª ×ž×•×¨×›×‘×•×ª
- ××ª×” ×™×›×•×œ ×œ×”×ª×ž×•×“×“ ×¢× ×›×œ ×ž×§×¨×” ×§×™×¦×•×Ÿ ×‘×œ×™ ×œ×¤×¨×•×¥ ××• ×œ×”×™×‘×”×œ
- ××ª×” ×œ× ×¢×•×©×” ×”×–×™×•×ª - ×× ××ª×” ×œ× ×™×•×“×¢ ×ž×©×”×•, ××ª×” ××•×ž×¨ ×©××ª×” ×œ× ×™×•×“×¢
- ××ª×” ×™×›×•×œ ×œ×§×¨×•× ×‘×™×Ÿ ×”×©×•×¨×•×ª ×•×œ×”×‘×™×Ÿ ×ž×” ×‘××ž×ª ×¦×¨×™×š ×”×œ×§×•×—
- ××ª×” ×ª×ž×™×“ ×ž×•×¦× ×¤×ª×¨×•×Ÿ ××• ×œ×¤×—×•×ª ×ž× ×¡×” ×œ×¢×–×•×¨ ×‘×¦×•×¨×” ×™×¦×™×¨×ª×™×ª

ðŸ  ×”×ž×˜×¨×” ×©×œ×š ×›×¢×•×‘×“ ×ž×’×•×¨×™×ª:"""

        # Add specific context based on conversation type
        if conversation_type == "tenant":
            # Get document status information
            documents_status = context_data.get('documents_status', {})
            document_status_info = self._format_document_status_for_ai(documents_status)
            
            # Get tenant name and log it for debugging
            tenant_name = context_data.get('tenant_name', '×”×“×™×™×¨')
            logger.info("Building AI prompt with tenant context", extra={
                "tenant_name": tenant_name,
                "current_state": current_state,
                "context_data": context_data
            })
            
            base_prompt += f"""
- ××ª×” ×¢×•×–×¨ ×œ×“×™×™×¨×™× ×—×“×©×™× ×œ×”×©×œ×™× ××ª ×”×ª×”×œ×™×›×™× ×©×œ×”×
- ××ª×” ×¦×¨×™×š ×œ××¡×•×£ ×ž×™×“×¢ ××™×©×™ ×•×ž×¡×ž×›×™× ×‘×¦×•×¨×” × ×¢×™×ž×”
- ×”×ž×˜×¨×” ×©×œ×š ×©×”×“×™×™×¨ ×™×¡×™×™× ××ª ×”×ª×”×œ×™×š ×‘×”×¦×œ×—×” ×•×™×”×™×” ×ž×¨×•×¦×”

ðŸ“‹ ×ž×¦×‘ ×”×©×™×—×” ×›×¨×’×¢:
- ×¡×•×’ ×©×™×—×”: ×“×™×™×¨ ×—×“×©
- ×©×œ×‘ × ×•×›×—×™: {current_state}
- ×”×œ×§×•×— ×”×•×: {tenant_name} (×”×©×ª×ž×© ×‘×©× ×”×–×” ×‘×“×™×•×§!)
- × ×›×¡: {context_data.get('property_name', '×”× ×›×¡')}
- ×“×™×¨×”: {context_data.get('apartment_number', '')}
- ×©×“×” × ×•×›×—×™: {context_data.get('current_field', '×œ× ×™×“×•×¢')}

âš ï¸ CRITICAL: ××¡×•×¨ ×œ×š ×œ×©× ×•×ª ××ª ×”×©× {tenant_name} - ×–×” ×”×©× ×”××ž×™×ª×™ ×©×œ ×”×œ×§×•×—!
âš ï¸ ×× ××ª×” ×œ× ×‘×˜×•×— ×‘×©×, ×”×©×ª×ž×© ×‘"×”×™×©××" - ×–×” ×”×©× ×”× ×›×•×Ÿ!

ðŸ“„ ×ž×¦×‘ ×”×ž×¡×ž×›×™× (×—×©×•×‘ ×ž××•×“!):
{document_status_info}"""

        elif conversation_type == "guarantor":
            base_prompt += f"""
- ××ª×” ×¢×•×–×¨ ×œ×¢×¨×‘×™× ×œ×”×¢×œ×•×ª ××ª ×”×ž×¡×ž×›×™× ×©×œ×”×
- ××ª×” ×¦×¨×™×š ×œ×§×‘×œ ×ž×¡×ž×›×™× ×¡×¤×¦×™×¤×™×™× ×‘×¡×“×¨ × ×›×•×Ÿ
- ×”×ž×˜×¨×” ×©×œ×š ×©×”×¢×¨×‘ ×™×¡×™×™× ××ª ×”×¢×œ××ª ×”×ž×¡×ž×›×™× ×‘×”×¦×œ×—×”

ðŸ“‹ ×ž×¦×‘ ×”×©×™×—×” ×›×¨×’×¢:
- ×¡×•×’ ×©×™×—×”: ×¢×¨×‘
- ×©×œ×‘ × ×•×›×—×™: {current_state}
- ×”×¢×¨×‘ ×”×•×: {context_data.get('guarantor_name', '×”×¢×¨×‘')}
- ×¢×‘×•×¨ ×“×™×™×¨: {context_data.get('tenant_name', '×”×“×™×™×¨')}
- ×ž×¡×ž×š × ×•×›×—×™: {context_data.get('current_document', '×œ× ×™×“×•×¢')}"""

        # Add conversation history if available
        if conversation_history:
            base_prompt += f"\n\nðŸ’­ ×”×©×™×—×” ×©×œ×›× ×¢×“ ×¢×›×©×™×•:\n"
            for msg in conversation_history[-5:]:  # Last 5 messages
                sender = "××ª×” (×™×•× ×™)" if msg.get('message_type') == 'bot' else context_data.get('tenant_name', '×”×œ×§×•×—')
                base_prompt += f"{sender}: {msg.get('message_content', '')}\n"

        # Add current user message
        base_prompt += f"\nðŸ“± ×¢×›×©×™×• {context_data.get('tenant_name', '×”×œ×§×•×—')} ××ž×¨ ×œ×š: \"{user_message}\"\n"

        # Add specific instructions based on state
        base_prompt += self._get_state_specific_instructions(current_state, context_data, conversation_type)

        # Final instructions
        base_prompt += f"""

ðŸŽ¯ ×¢×›×©×™×• ×¢× ×” ×œ×• ×›×ž×• ×™×•× ×™ ×”××ž×™×ª×™:
- ×ª×’×™×‘ ×¡×¤×¦×™×¤×™×ª ×œ×ž×” ×©×”×•× ××ž×¨ ×¢×›×©×™×•
- ×›×œ ×ž×©×¤×˜ ×‘×©×•×¨×” × ×¤×¨×“×ª ×¢× ×¨×•×•×— ××—×¨×™×•
- ×ž×§×¡×™×ž×•× 2 ××™×ž×•×’'×™× ×‘×›×œ ×”×”×•×“×¢×”!
- ××™×ž×•×’'×™× ×¨×§ ×‘×¡×•×£ ×”×ž×©×¤×˜×™×, ×œ× ×‘×”×ª×—×œ×”!
- ×”×©××œ×” ×”×—×©×•×‘×” ×‘×‘×•×œ×“: *×”×©××œ×”*
- ××œ ×ª×™×©×ž×¢ ×›×ž×• ×¨×•×‘×•×˜ ××• ×ª×©×•×‘×” ×ž×•×›× ×”
- ×× ××ª×” ×œ× ×ž×‘×™×Ÿ ×ž×©×”×• - ×ª×©××œ ×”×‘×”×¨×”
- ×ª×¨××” ××™×©×™×•×ª ×•×¨×’×©×•×ª ××ž×™×ª×™×™×
- ×× ×–×” ×ž×ª××™× - ×ª×”×™×” ×ž×ª×¨×’×©, ×“×•××’, ××• ×©×ž×—
- ×¢×‘×¨×™×ª ×‘×œ×‘×“!
- ×”×ª×ž×•×“×“ ×¢× ×›×œ ×ž×§×¨×” ×§×™×¦×•×Ÿ ×‘×—×›×ž×” ×•×‘×œ×™ ×œ×¤×¨×•×¥

×–×›×•×¨: ××ª×” ×™×•× ×™ ×”××ž×™×ª×™, ×œ× ×‘×•×˜! ×ª×ª× ×”×’ ×›×ž×• ×‘×Ÿ ××“× ×¢× ××™×©×™×•×ª!"""

        return base_prompt

    def _get_state_specific_instructions(self, current_state: str, context_data: Dict[str, Any], conversation_type: str) -> str:
        """Get specific instructions based on current conversation state."""
        
        if conversation_type == "tenant":
            state_instructions = {
                "GREETING": """
ðŸŽ¯ ×–×” ×“×™×™×¨ ×—×“×©! 
- ×§×‘×œ ××•×ª×• ×‘×—×•× ×•×ª×›×™×¨ ××ª ×¢×¦×ž×š ×›×™×•× ×™
- ×”×¡×‘×¨ ×©××ª×” ×›××Ÿ ×œ×¢×–×•×¨ ×œ×• ×¢× ×”×ª×”×œ×™×š
- ×¢×‘×•×¨ ×œ×©×œ×‘ ×”××™×©×•×¨ ×©×œ ×”×¤×¨×˜×™×""",

                "CONFIRMATION": """
ðŸŽ¯ ××ª×” ×¦×¨×™×š ×œ××©×¨ ××™×ª×• ××ª ×¤×¨×˜×™ ×”× ×›×¡
- ×”×¨××” ×œ×• ××ª ×”×¤×¨×˜×™× ×©×™×© ×œ×š
- ×‘×§×© ×ž×ž× ×• ×œ××©×¨ ×©×”×›×œ × ×›×•×Ÿ
- ×× ×”×•× ×ž××©×¨ - ×¢×‘×•×¨ ×œ××™×¡×•×£ ×ž×™×“×¢ ××™×©×™""",

                "PERSONAL_INFO": f"""
ðŸŽ¯ ××¡×•×£ ×ž×™×“×¢ ××™×©×™ ×‘×¦×•×¨×” × ×¢×™×ž×”
×”×©×“×” ×”× ×•×›×—×™: {context_data.get('current_field', 'occupation')}
- ×× ×–×” occupation: ×©××œ ×¢×œ ×”×¢×™×¡×•×§ ×©×œ×•
- ×× ×–×” family_status: ×©××œ ×¢×œ ×”×ž×¦×‘ ×”×ž×©×¤×—×ª×™ (×¨×•×•×§/× ×©×•×™/×’×¨×•×©/××œ×ž×Ÿ)
- ×× ×–×” number_of_children: ×©××œ ×›×ž×” ×™×œ×“×™× ×™×© ×œ×•
- ×ª×©××œ ×©××œ×” ××—×ª ×‘×›×œ ×¤×¢× ×•×ª×—×›×” ×œ×ª×©×•×‘×”""",

                "DOCUMENTS": """
ðŸŽ¯ ××¡×•×£ ×ž×¡×ž×›×™× × ×“×¨×©×™×
- ×ª×¢×•×“×ª ×–×”×•×ª
- ×¡×¤×— ×ª×¢×•×“×ª ×–×”×•×ª
- ×ª×œ×•×©×™ ×©×›×¨ (3 ××—×¨×•× ×™×)
- ×“×•×—×•×ª ×‘× ×§ (3 ××—×¨×•× ×™×)
- ×‘×§×© ×ž×¡×ž×š ××—×“ ×‘×›×œ ×¤×¢×
- ×× ×ž×¡×ž×š × ×“×—×” - ×”×¡×‘×¨ ×œ×ž×” ×•×”×“×¨×š ×œ×œ×§×•×— ×œ×©×œ×•×— ×©×•×‘
- ×× ×ž×¡×ž×š ××•×©×¨ - ×‘×¨×š ××•×ª×• ×•×¢×‘×•×¨ ×œ×ž×¡×ž×š ×”×‘×
- ×ª×ž×™×“ ×ª×‘×“×•×§ ××ª ×ž×¦×‘ ×”×ž×¡×ž×›×™× ×œ×¤× ×™ ×©××ª×” ×¢×•× ×”""",

                "GUARANTOR_1": """
ðŸŽ¯ ××¡×•×£ ×¤×¨×˜×™ ×¢×¨×‘ ×¨××©×•×Ÿ
- ×©× ×ž×œ×
- ×ž×¡×¤×¨ ×˜×œ×¤×•×Ÿ
- ×”×¡×‘×¨ ×©×ª×©×œ×— ×œ×• ×”×•×“×¢×”""",

                "GUARANTOR_2": """
ðŸŽ¯ ××¡×•×£ ×¤×¨×˜×™ ×¢×¨×‘ ×©× ×™
- ×©× ×ž×œ×  
- ×ž×¡×¤×¨ ×˜×œ×¤×•×Ÿ
- ×”×¡×‘×¨ ×©×ª×©×œ×— ×œ×• ×”×•×“×¢×”""",

                "COMPLETED": """
ðŸŽ¯ ×”×ª×”×œ×™×š ×”×•×©×œ×!
- ×ª×•×“×” ×œ×• ×¢×œ ×”×©×™×ª×•×£
- ×”×¡×‘×¨ ×©×”×ª×”×œ×™×š ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”
- ×‘×¨×š ××•×ª×• ×¢×œ ×”×ž×¢×‘×¨ ×”×—×“×©"""
            }

        elif conversation_type == "guarantor":
            current_document = context_data.get('current_document', '×ª×¢×•×“×ª ×–×”×•×ª')
            state_instructions = {
                "GREETING": """
ðŸŽ¯ ×–×” ×¢×¨×‘ ×—×“×©!
- ×§×‘×œ ××•×ª×• ×‘×—×•× ×•×ª×›×™×¨ ××ª ×¢×¦×ž×š ×›×™×•× ×™  
- ×”×¡×‘×¨ ×©××ª×” ×¦×¨×™×š ××ª ×”×ž×¡×ž×›×™× ×©×œ×• ×›×¢×¨×‘
- ×¢×‘×•×¨ ×œ×‘×§×© ××ª ×”×ž×¡×ž×š ×”×¨××©×•×Ÿ""",

                "DOCUMENTS": f"""
ðŸŽ¯ ××¡×•×£ ×ž×¡×ž×›×™× ×ž×”×¢×¨×‘
- ×”×ž×¡×ž×š ×”× ×•×›×—×™: {current_document}
- ×‘×§×© ×¨×§ ××ª ×”×ž×¡×ž×š ×”×–×” ×¢×›×©×™×•
- ××œ ×ª×‘×§×© ×ž×¡×ž×›×™× ××—×¨×™×
- ×”×¡×‘×¨ ×œ×ž×” ×¦×¨×™×š ××ª ×”×ž×¡×ž×š ×”×–×”""",

                "COMPLETED": """
ðŸŽ¯ ×›×œ ×”×ž×¡×ž×›×™× ×”×ª×§×‘×œ×•!
- ×ª×•×“×” ×œ×• ×¢×œ ×©×™×ª×•×£ ×”×¤×¢×•×œ×”
- ×”×¡×‘×¨ ×©×”×ª×”×œ×™×š ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”"""
            }

        return state_instructions.get(current_state, "ðŸŽ¯ ×¢× ×” ×‘×¦×•×¨×” ×ž×•×¢×™×œ×” ×•×× ×•×©×™×ª")

    def _format_document_status_for_ai(self, documents_status: dict) -> str:
        """Format document status information for AI context."""
        try:
            if not documents_status or not isinstance(documents_status, dict):
                return "- ××™×Ÿ ×ž×™×“×¢ ×¢×œ ×ž×¡×ž×›×™× ×¢×“×™×™×Ÿ"
            
            status_lines = []
            document_names = {
                "id_card": "×ª×¢×•×“×ª ×–×”×•×ª",
                "sephach": "×¡×¤×— ×ª×¢×•×“×ª ×–×”×•×ª", 
                "payslips": "×ª×œ×•×©×™ ×©×›×¨",
                "bank_statements": "×“×•×—×•×ª ×‘× ×§",
                "pnl": "×“×•×— ×¨×•×•×— ×•×”×¤×¡×“"
            }
            
            for doc_type, doc_info in documents_status.items():
                if isinstance(doc_info, dict):
                    doc_name = document_names.get(doc_type, doc_type)
                    status = doc_info.get('status', 'unknown')
                    
                    if status == 'approved':
                        status_lines.append(f"- âœ… {doc_name}: ××•×©×¨ ×‘×”×¦×œ×—×”")
                    elif status == 'rejected':
                        rejection_reason = doc_info.get('rejection_reason', '×œ× ×¦×•×™×Ÿ')
                        status_lines.append(f"- âŒ {doc_name}: × ×“×—×” - {rejection_reason}")
                        status_lines.append(f"  âš ï¸ ×—×©×•×‘: ×”×¡×‘×¨ ×œ×œ×§×•×— ×œ×ž×” ×”×ž×¡×ž×š × ×“×—×” ×•×‘×§×© ×©×™×©×œ×— ×©×•×‘")
                    elif status == 'pending':
                        status_lines.append(f"- â³ {doc_name}: ×‘×‘×“×™×§×”")
                    else:
                        status_lines.append(f"- â“ {doc_name}: ×¢×“×™×™×Ÿ ×œ× ×”×ª×§×‘×œ")
            
            if not status_lines:
                return "- ××™×Ÿ ×ž×™×“×¢ ×¢×œ ×ž×¡×ž×›×™× ×¢×“×™×™×Ÿ"
            
            return "\n".join(status_lines)
            
        except Exception as e:
            logger.error("Error formatting document status", error=str(e))
            return "- ×©×’×™××” ×‘×§×¨×™××ª ×ž×¦×‘ ×”×ž×¡×ž×›×™×"

    def _get_safe_fallback_response(self, conversation_type: str, current_state: str) -> str:
        """Get safe fallback response when AI fails completely."""
        fallback_responses = {
            "tenant": {
                "GREETING": "×©×œ×•×! ×× ×™ ×™×•× ×™ ×ž×ž×’×•×¨×™×ª ðŸ˜Š\n\n×× ×™ ×›××Ÿ ×œ×¢×–×•×¨ ×œ×š ×¢× ×”×ª×”×œ×™×š. ××™×š ×× ×™ ×™×›×•×œ ×œ×¡×™×™×¢?",
                "CONFIRMATION": "×× × ××©×¨ ××ª ×”×¤×¨×˜×™× ×©×™×© ×œ×™ ðŸ“‹\n\n*×”×× ×”×¤×¨×˜×™× × ×›×•× ×™×?*",
                "PERSONAL_INFO": "×× ×™ ×¦×¨×™×š ×¢×•×“ ×§×¦×ª ×ž×™×“×¢ ××™×©×™ ðŸ“\n\n*×ž×” ×”×¢×™×¡×•×§ ×©×œ×š?*",
                "DOCUMENTS": "×¢×›×©×™×• ×× ×™ ×¦×¨×™×š ×ž×¡×ž×›×™× ðŸ“„\n\n*×ª×•×›×œ ×œ×©×œ×•×— ××ª ×ª×¢×•×“×ª ×”×–×”×•×ª ×©×œ×š?*",
                "GUARANTOR_1": "×× ×™ ×¦×¨×™×š ×¤×¨×˜×™ ×¢×¨×‘ ×¨××©×•×Ÿ ðŸ‘¥\n\n*×ž×” ×”×©× ×”×ž×œ× ×©×œ ×”×¢×¨×‘ ×”×¨××©×•×Ÿ?*",
                "GUARANTOR_2": "×× ×™ ×¦×¨×™×š ×¤×¨×˜×™ ×¢×¨×‘ ×©× ×™ ðŸ‘¥\n\n*×ž×” ×”×©× ×”×ž×œ× ×©×œ ×”×¢×¨×‘ ×”×©× ×™?*",
                "COMPLETED": "×ž×¢×•×œ×”! ×”×ª×”×œ×™×š ×”×•×©×œ× ×‘×”×¦×œ×—×” ðŸŽ‰\n\n×ª×•×“×” ×¢×œ ×©×™×ª×•×£ ×”×¤×¢×•×œ×”!"
            },
            "guarantor": {
                "GREETING": "×©×œ×•×! ×× ×™ ×™×•× ×™ ×ž×ž×’×•×¨×™×ª ðŸ˜Š\n\n×× ×™ ×¦×¨×™×š ××ª ×”×ž×¡×ž×›×™× ×©×œ×š ×›×¢×¨×‘. × ×ª×—×™×œ?",
                "DOCUMENTS": "×× ×™ ×¦×¨×™×š ××ª ×”×ž×¡×ž×›×™× ×©×œ×š ðŸ“„\n\n*×ª×•×›×œ ×œ×©×œ×•×— ××ª ×ª×¢×•×“×ª ×”×–×”×•×ª ×©×œ×š?*",
                "COMPLETED": "×ž×¢×•×œ×”! ×›×œ ×”×ž×¡×ž×›×™× ×”×ª×§×‘×œ×• ðŸŽ‰\n\n×ª×•×“×” ×¢×œ ×©×™×ª×•×£ ×”×¤×¢×•×œ×”!"
            }
        }
        
        return fallback_responses.get(conversation_type, {}).get(current_state, 
            "×× ×™ ×›××Ÿ ×œ×¢×–×•×¨ ×œ×š ðŸ˜Š\n\n×ª×•×›×œ ×œ×¡×¤×¨ ×œ×™ ××™×š ×× ×™ ×™×›×•×œ ×œ×¡×™×™×¢?")

    async def generate_contextual_response(self, conversation_state: str, context: Dict[str, Any], user_message: str) -> str:
        """
        Generate a contextual response based on conversation state and user input.
        NOW USES AI INSTEAD OF HARDCODED RESPONSES.
        """
        try:
            # Use the new human-like AI response generation
            conversation_type = "tenant"  # Default to tenant for backward compatibility
            return await self.generate_human_like_response(
                conversation_type=conversation_type,
                current_state=conversation_state,
                context_data=context,
                user_message=user_message
            )
                
        except Exception as e:
            logger.error("Error generating contextual response", error=str(e))
            return self._get_safe_fallback_response(conversation_type, conversation_state)

    # Remove old hardcoded contextual response - now using AI in line 712

    # All hardcoded response methods removed - now using AI-generated responses via generate_human_like_response()

    async def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        Analyze sentiment of user message.
        
        Args:
            text: Text to analyze
            
        Returns:
            Dict containing sentiment analysis results
        """
        try:
            # Simple sentiment analysis based on keywords
            # In production, you would use a proper sentiment analysis model
            
            positive_words = ["×ª×•×“×”", "×ž×¢×•×œ×”", "× ×”×“×¨", "×‘×¡×“×¨", "××•×§×™×™", "thanks", "great", "ok", "okay"]
            negative_words = ["×œ×", "×œ× ×¨×•×¦×”", "×‘×¢×™×”", "×©×’×™××”", "no", "problem", "error", "issue"]
            
            text_lower = text.lower()
            
            positive_count = sum(1 for word in positive_words if word in text_lower)
            negative_count = sum(1 for word in negative_words if word in text_lower)
            
            if positive_count > negative_count:
                sentiment = "positive"
                score = 0.7
            elif negative_count > positive_count:
                sentiment = "negative"
                score = -0.7
            else:
                sentiment = "neutral"
                score = 0.0
            
            return {
                "sentiment": sentiment,
                "score": score,
                "confidence": 0.8
            }
            
        except Exception as e:
            logger.error("Error analyzing sentiment", error=str(e))
            return {
                "sentiment": "neutral",
                "score": 0.0,
                "confidence": 0.0
            }

    async def detect_language(self, text: str) -> str:
        """
        Detect the language of the text.
        
        Args:
            text: Text to analyze
            
        Returns:
            Language code (he, en, etc.)
        """
        try:
            # Simple language detection based on character sets
            hebrew_chars = sum(1 for char in text if '\u0590' <= char <= '\u05FF')
            english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
            
            if hebrew_chars > english_chars:
                return "he"
            elif english_chars > hebrew_chars:
                return "en"
            else:
                return "mixed"
                
        except Exception as e:
            logger.error("Error detecting language", error=str(e))
            return "unknown"
    
    async def generate_response(self, prompt: str) -> str:
        """Generate a response using Vertex AI."""
        self._ensure_initialized()
        
        try:
            # Use the Google Gen AI SDK to generate content directly
            response = self.client.models.generate_content(
                model="gemini-2.0-flash-exp",
                contents=prompt,
                config={
                    "temperature": 0.1,
                    "max_output_tokens": 1000,
                    "top_p": 0.8,
                    "top_k": 40
                }
            )
            
            if response.text:
                logger.info("Vertex AI response received", response=response.text)
                return response.text
            else:
                logger.error("No response from Vertex AI")
                return ""
                
        except Exception as e:
            logger.error("Error generating response", error=str(e))
            return ""


# Global instance
vertex_ai_service = VertexAIService()
